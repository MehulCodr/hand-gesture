{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import pathlib\n",
    "import tempfile\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBdpCS6P8xdphp7768Jy6AclyjJpuGcvGs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "prompt = \"Generate a transcript of the speech.\"\n",
    "\n",
    "def stt():\n",
    "    response = model.generate_content([\n",
    "        prompt,\n",
    "        {\n",
    "            \"mime_type\": \"audio/ogg\",\n",
    "            \"data\": pathlib.Path('yappin.ogg').read_bytes()\n",
    "        }\n",
    "    ])\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    }
   ],
   "source": [
    "# Initialize VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(1)  # 1 is low, 2 is medium, 3 is high sensitivity\n",
    "\n",
    "# Function to process audio and return transcript\n",
    "def process_audio_chunk(audio_data):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".ogg\", delete=False) as temp_file:\n",
    "        temp_file.write(audio_data)\n",
    "        response = model.generate_content([\n",
    "            prompt,\n",
    "            {\n",
    "                \"mime_type\": \"audio/ogg\",\n",
    "                \"data\": pathlib.Path(temp_file.name).read_bytes()\n",
    "            }\n",
    "        ])\n",
    "    return response.text\n",
    "\n",
    "# Function to execute actions based on commands\n",
    "def execute_command(transcript):\n",
    "    if \"turn on the lights\" in transcript:\n",
    "        print(\"Turning on the lights!\")\n",
    "        # Add your logic to turn on the lights\n",
    "    elif \"play music\" in transcript:\n",
    "        print(\"Playing music!\")\n",
    "        # Add your logic to play music\n",
    "\n",
    "# Function to check if audio contains speech\n",
    "def is_speech(frame, sample_rate):\n",
    "    frame_length = 320  # 20 ms of audio for 16kHz\n",
    "    if len(frame) != frame_length:\n",
    "        raise ValueError(f\"Expected frame length {frame_length}, but got {len(frame)}\")\n",
    "    return vad.is_speech(frame, sample_rate)\n",
    "\n",
    "# Continuous listening\n",
    "def listen_and_process():\n",
    "    rate = 16000  # 16 kHz sample rate\n",
    "    chunk_size = 320  # 20 ms of audio, 320 samples at 16 kHz\n",
    "    silence_duration = 0\n",
    "    silence_threshold = 1.0  # seconds of silence to stop recording\n",
    "    audio_frames = []  # renamed to avoid conflict\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        nonlocal silence_duration, audio_frames\n",
    "        if status:\n",
    "            print(\"Status:\", status)\n",
    "\n",
    "        # Convert audio data to an array (16-bit PCM)\n",
    "        audio_array = np.frombuffer(indata, dtype=np.int16)\n",
    "\n",
    "        # Ensure the frame is of correct length for VAD (20 ms of audio)\n",
    "        if len(audio_array) != chunk_size:\n",
    "            return  # Skip frames that don't match 320 samples\n",
    "\n",
    "        # Check for speech in the audio frame\n",
    "        try:\n",
    "            if is_speech(audio_array.tobytes(), rate):\n",
    "                print(\"Speech detected!\")\n",
    "                silence_duration = 0  # reset silence duration if speech is detected\n",
    "            else:\n",
    "                silence_duration += chunk_size / rate\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        # Accumulate frames for processing\n",
    "        audio_frames.append(indata)\n",
    "\n",
    "        # Stop recording if silence duration exceeds the threshold\n",
    "        if silence_duration > silence_threshold and audio_frames:\n",
    "            print(\"Silence detected, processing audio...\")\n",
    "            audio_data = b''.join(audio_frames)\n",
    "            transcript = process_audio_chunk(audio_data)\n",
    "            print(\"Transcript:\", transcript)\n",
    "            execute_command(transcript)\n",
    "            audio_frames = []  # reset frames\n",
    "            silence_duration = 0  # reset silence duration\n",
    "\n",
    "    # Set up the stream for recording\n",
    "    with sd.InputStream(callback=callback, channels=1, samplerate=rate, blocksize=chunk_size):\n",
    "        print(\"Listening...\")\n",
    "        try:\n",
    "            sd.sleep(-1)  # Keep the stream running indefinitely\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stream stopped by user.\")\n",
    "\n",
    "# Run the listening function\n",
    "listen_and_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
